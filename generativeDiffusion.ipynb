{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Generative Diffusion Model with WoW Icons\n",
    "\n",
    "## Project Objectives\n",
    "\n",
    "In this notebook, I'll document the process of building a generative diffusion model that can create new WoW-style icons (kind of).\n",
    "\n",
    "This implementation uses a simple DDPM (Denoising Diffusion Probabilistic Model) approach. The model exclusively removes noise based on random input noise images, without any conditioning or optimizations. In fact, it's entirely unoptimized for running on my laptop's limited hardware, so this notebook is really a learning exercise to help me understand the fundamentals of diffusion models.\n",
    "\n",
    "## Process Overview\n",
    "\n",
    "<img src=\"assets/Diffusion%20Model%20Pytorch.webp\" alt=\"Diffusion Model PyTorch\" style=\"width:700px; height:auto;\"><br>\n",
    "<i>(['Understanding Deep Learning'](https://mitpress.mit.edu/9780262048644/understanding-deep-learning/) by Simon J.D. Prince via ['Diffusion Model from Scratch in Pytorch'](https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946/) by Nicholas DiSalvo)</i>\n",
    "\n",
    "Diffusion models work by gradually adding noise to images (forward process) and then learning to reverse this process (reverse process), allowing for high-quality image generation.\n",
    "\n",
    "The forward process is a predetermined 'Markov chain' based on a noise schedule. Whereby; \"predetermined\" means we decide in advance exactly how much noise to add at each step according to our noise schedule - it's not random but follows a specific pattern, and \"Markov chain\" means each noisy version only depends on the previous step - like a chain where each link only connects to the ones right next to it. When you combine these concepts, you can precisely calculate how noisy an image will be at any step in the process, setting the stage for a model to learn how to remove the noise that has been added.\n",
    "\n",
    "<img src=\"assets/unet-model-diagram.webp\" alt=\"Unet Model\" style=\"width:500px; height:auto;\"><br>\n",
    "<i>(UNET for Diffusion by [Nicholas DiSalvo](https://towardsdatascience.com/author/nickd16718/))</i>\n",
    "\n",
    "A U-Net architecture is trained to predict the noise that was added at any arbitrary timestep, by taking a noisy image at a specific timestep and trying to predict what noise was added to it.<br>The U-Net architecture itself, with its downsampling path (reduces spatial dimensions) and upsampling (increases spatial dimensions) path connected by skip connections, is particularly good at this task because:\n",
    "\n",
    "- The downsampling captures the overall structure and context\n",
    "- The upsampling reconstructs detailed features\n",
    "- The skip connections help preserve important information from earlier layers\n",
    "\n",
    "It could be optimised with scoring or conditional inputs, but we're keeping things simple for now.\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "For this project, I'm using the \"CleanIcons-MechagnomeEdition\" dataset, specifically version 11.1.0.59347-V4-1, which contains a collection of World of Warcraft icons (upscaled by Mechagnome).\n",
    "This dataset provides a foundation for exploring diffusion model training on stylistically consistent gaming assets - It may genuinely not work at all, but let's see...\n",
    "\n",
    "### Dataset Details\n",
    "\n",
    "- **Name**: CleanIcons-MechagnomeEdition-11.1.0.59347-V4-1\n",
    "- **Source**: [GitHub - AcidWeb/Clean-Icons-Mechagnome-Edition](https://github.com/AcidWeb/Clean-Icons-Mechagnome-Edition/releases/tag/11.1.0.59347-V4-1)\n",
    "- **Contents**: World of Warcraft icons\n",
    "- **Format**: TGA (Truevision Graphics Adapter) image files\n",
    "- **Features**: High-quality game icons with transparency support\n",
    "\n",
    "### Implementation Notes\n",
    "\n",
    "This implementation adapts the diffusion model architecture to work with:\n",
    "\n",
    "- TGA format images (using PIL for loading)\n",
    "- Apple Silicon hardware (ensuring MPS device compatibility)\n",
    "- Custom data loading pipeline for WoW icon processing\n",
    "\n",
    "### Credits and References\n",
    "\n",
    "This implementation takes direct reference from [\"A Diffusion Model from Scratch in PyTorch\"](https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing#scrollTo=Rj17psVw7Shg).\n",
    "\n",
    "The original work references:\n",
    "\n",
    "- GitHub implementation of [Denoising Diffusion PyTorch](https://github.com/lucidrains/denoising-diffusion-pytorch)\n",
    "- Work by Niels Rogge and Kashif Rasul in their [Huggingface notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=3a159023)\n",
    "- Academic papers on Diffusion models:\n",
    "  - [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672) (Nichol & Dhariwal, 2021)\n",
    "  - [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (Ho et al., 2020)\n",
    "  - [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233) (Dhariwal & Nichol, 2021)\n",
    "\n",
    "I've adapted this implementation to work specifically with TGA format WoW icons on Apple Silicon hardware, modifying the data loading pipeline and ensuring MPS device compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display random samples from dataset\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "icons_list = os.listdir(\"./data/ICONS\")\n",
    "icons_list_sample = np.random.choice(icons_list, size=15)\n",
    "\n",
    "for i, item in enumerate(icons_list_sample):\n",
    "    icon_path = os.path.join(\"./data/ICONS\", item)\n",
    "    icon_image = Image.open(icon_path)\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(icon_image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Random Samples from Dataset\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Diffusion Process Explained\n",
    "\n",
    "The forward diffusion process gradually adds noise to an image over a series of timesteps. Let's break down the key components:\n",
    "\n",
    "### Noise Schedule Function\n",
    "\n",
    "```python\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "```\n",
    "\n",
    "This function creates a linear schedule for the noise variance (β) at each timestep:\n",
    "\n",
    "- Starts with minimal noise (`start=0.0001`)\n",
    "- Gradually increases to larger noise (`end=0.02`)\n",
    "- Creates `timesteps` evenly spaced values\n",
    "- The schedule controls how quickly information is destroyed during diffusion\n",
    "\n",
    "### Helper Function for Batch Processing\n",
    "\n",
    "```python\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "```\n",
    "\n",
    "This utility function:\n",
    "\n",
    "- Extracts specific values from a parameter tensor based on timestep indices\n",
    "- Reshapes the output to properly broadcast across the image dimensions\n",
    "- Ensures values are on the correct device for computation\n",
    "- Enables efficient batch processing of images at different timesteps\n",
    "\n",
    "### Forward Diffusion Function\n",
    "\n",
    "```python\n",
    "def forward_diffusion_sample(x_0, t, device=\"mps\"):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(\n",
    "        device\n",
    "    ) + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "```\n",
    "\n",
    "This implements the core diffusion equation from Ho et al. (2020):\n",
    "\n",
    "$$q(x_t|x_0) = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\cdot \\epsilon$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x_0$ is the original image\n",
    "- $x_t$ is the noisy image at timestep $t$\n",
    "- $\\epsilon$ is random Gaussian noise\n",
    "- $\\bar{\\alpha}_t$ represents how much of the original signal remains at timestep $t$\n",
    "\n",
    "### Pre-calculated Diffusion Parameters\n",
    "\n",
    "```python\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "```\n",
    "\n",
    "These calculations:\n",
    "\n",
    "- Set up the full noise schedule for all timesteps\n",
    "- Pre-compute constants needed for both forward diffusion and the eventual reverse process\n",
    "- Define how the signal-to-noise ratio changes at each step of the diffusion\n",
    "- Calculate the posterior variance needed for the sampling process\n",
    "\n",
    "The mathematical setup follows the DDPM paper (Ho et al., 2020), where:\n",
    "\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{i=1}^{t} \\alpha_i$\n",
    "\n",
    "These pre-calculations allow for efficient training and sampling as we don't need to recompute these values repeatedly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"mps\"):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(\n",
    "        device\n",
    "    ) + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Data Loading Pipeline\n",
    "\n",
    "This section sets up the data loading pipeline for our WoW icons dataset. <br>\n",
    "It handles loading the TGA image files, preprocessing them, and creating batches for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Check for MPS availability\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "IMG_SIZE = 16\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "# Custom transforms that can be pickled (unlike lambda functions)\n",
    "class NormalizeNegOneToOne(object):\n",
    "    def __call__(self, x):\n",
    "        return (x * 2) - 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class PermuteCHWToHWC(object):\n",
    "    def __call__(self, x):\n",
    "        return x.permute(1, 2, 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class ToNumpy(object):\n",
    "    def __call__(self, x):\n",
    "        return x.numpy().astype(np.uint8)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class WoWIconsDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with the TGA files\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.file_list = [f for f in os.listdir(root_dir) if f.endswith(\".tga\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.file_list[idx])\n",
    "        # PIL can open TGA files\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def load_transformed_dataset(data_dir):\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),  # Scales data into [0,1]\n",
    "        NormalizeNegOneToOne(),  # Scale between [-1, 1]\n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    dataset = WoWIconsDataset(root_dir=data_dir, transform=data_transform)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    # First move tensor to CPU before transformations\n",
    "    if image.device.type != \"cpu\":\n",
    "        image = image.cpu()\n",
    "\n",
    "    reverse_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(\n",
    "                lambda t: (t + 1) / 2\n",
    "            ),  # Safe to use Lambda here as it's not in DataLoader\n",
    "            PermuteCHWToHWC(),  # CHW to HWC\n",
    "            transforms.Lambda(\n",
    "                lambda t: t * 255.0\n",
    "            ),  # Safe to use Lambda here as it's not in DataLoader\n",
    "            ToNumpy(),\n",
    "            transforms.ToPILImage(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :]\n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "\n",
    "# Assuming your extracted TGA files are in a directory called \"wow_icons\"\n",
    "data_dir = \"./data/ICONS\"\n",
    "data = load_transformed_dataset(data_dir)\n",
    "\n",
    "# Configure DataLoader with zero workers for now to avoid pickle issues\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,  # Use single process for now to avoid pickle errors\n",
    ")\n",
    "\n",
    "\n",
    "# Function to move batch to device efficiently\n",
    "def prepare_batch(batch, target_device=device):\n",
    "    return batch.to(target_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Diffusion Demo\n",
    "\n",
    "This visualization displays the forward diffusion process applied to our WoW icons. Each row shows a different randomly selected icon from the dataset, while columns represent progressive timesteps in the diffusion process.<br><br>\n",
    "Moving from left to right along each row, you see how the diffusion model gradually adds more noise to the original clean images. The leftmost column shows the original, unmodified WoW icons in their clean state. As you move right, each subsequent image shows the same icon with increasing amounts of random noise applied according to a carefully controlled schedule (T=300).<br><br>\n",
    "By the rightmost column, the icons have been almost completely transformed into pure noise, with little to no recognizable features remaining from the original images. This visual demonstration illustrates how the forward diffusion process systematically destroys information in the original images over time.<br><br>\n",
    "Understanding this forward process is crucial because training a diffusion model involves teaching a neural network to reverse this exact process - starting from pure noise and progressively removing it to generate new, realistic WoW icons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_icons_with_diffusion(\n",
    "    dataloader, num_icons=3, num_steps=7, T=300, device=\"mps\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Display a grid of randomly selected icons with the diffusion process applied.\n",
    "    Each row shows one icon progressively diffused with noise from left to right.\n",
    "    The first column shows the original, unmodified icon.\n",
    "\n",
    "    Args:\n",
    "        dataloader: PyTorch dataloader containing the icon dataset\n",
    "        num_icons: Number of different icons to display (rows)\n",
    "        num_steps: Number of diffusion steps to display (not including original)\n",
    "        T: Total number of timesteps in the diffusion process\n",
    "        device: Device to run computations on\n",
    "    \"\"\"\n",
    "    # Get a batch of images\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    # Move batch to device\n",
    "    batch = prepare_batch(batch, device)\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=((num_steps + 1) * 2, num_icons * 2))\n",
    "\n",
    "    # Get random indices from the batch\n",
    "    batch_size = batch.shape[0]\n",
    "    indices = np.random.choice(batch_size, size=num_icons, replace=False)\n",
    "\n",
    "    # Calculate the step size between diffusion steps\n",
    "    stepsize = int(T / num_steps)\n",
    "\n",
    "    # Plot each selected image with different noise levels\n",
    "    for row, idx in enumerate(indices):\n",
    "        # Get the original image\n",
    "        original_img = batch[idx].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # First, display the original image with no noise\n",
    "        plt.subplot(num_icons, num_steps + 1, row * (num_steps + 1) + 1)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        if original_img.device.type != \"cpu\":\n",
    "            original_display = original_img.cpu()\n",
    "        else:\n",
    "            original_display = original_img\n",
    "\n",
    "        show_tensor_image(original_display)\n",
    "\n",
    "        # Add \"Original\" as title for the first row\n",
    "        if row == 0:\n",
    "            plt.title(\"Original\", fontsize=8)\n",
    "\n",
    "        # Apply different levels of noise for the remaining columns\n",
    "        for col, timestep in enumerate(range(0, T, stepsize)):\n",
    "            if col >= num_steps:\n",
    "                break\n",
    "\n",
    "            plt.subplot(\n",
    "                num_icons, num_steps + 1, row * (num_steps + 1) + col + 2\n",
    "            )  # +2 because we already used column 1\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Apply forward diffusion with current timestep\n",
    "            t = torch.tensor([timestep], device=device).type(torch.int64)\n",
    "            noisy_img, _ = forward_diffusion_sample(original_img, t, device=device)\n",
    "\n",
    "            # Display the image\n",
    "            if noisy_img.device.type != \"cpu\":\n",
    "                noisy_img = noisy_img.cpu()\n",
    "\n",
    "            show_tensor_image(noisy_img)\n",
    "\n",
    "            # Add timestep as title for the first row\n",
    "            if row == 0:\n",
    "                plt.title(f\"t={timestep}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "display_icons_with_diffusion(dataloader, num_icons=3, num_steps=7, T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reverse Diffusion Process and U-Net Architecture\n",
    "\n",
    "While the forward diffusion process gradually adds noise to images, the reverse diffusion process is where the magic bits happens - <br>It's how we generate new images by progressively removing noise. This is accomplished using a neural network (in our case, a U-Net) that predicts the noise component at each timestep.\n",
    "\n",
    "### The Reverse Diffusion Equation\n",
    "\n",
    "The diffusion model learns to reverse the forward process by predicting the noise ε that was added at each timestep. The mathematical equation for the reverse process is:\n",
    "\n",
    "$$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $p_\\theta$ is our model's reverse distribution\n",
    "- $\\mu_\\theta$ is the predicted mean\n",
    "- $\\Sigma_\\theta$ is the predicted variance\n",
    "\n",
    "Instead of trying to learn how to generate images directly (which is very hard), the neural network (U-Net) is trained on a simpler task: \"Given an image with noise at step t, can you predict what noise was added?\". Once the network is trained to predict noise accurately, we can use it to generate brand new images by starting with pure random noise and then iteratively \"removing\" the predicted noise.\n",
    "\n",
    "### U-Net Architecture Overview\n",
    "\n",
    "A U-Net is a specific type of neural network architecture that was originally developed for biomedical image segmentation but has become widely used across many image processing tasks, including diffusion models.\n",
    "The mechanism that makes U-Net effective for both medical image segmentation and diffusion models comes down to some key architectural properties that transfer well between these seemingly different tasks:\n",
    "\n",
    "- <b>Preserving spatial relationships</b>: In medical imaging, U-Net excels at identifying boundaries and structures while maintaining their spatial relationships. In diffusion models, this same ability helps preserve the underlying structure of an image while noise is being predicted or removed.\n",
    "- <b>Multi-scale feature processing</b>: Medical images often contain features at different scales (from tiny cell structures to larger organs). Similarly, noise in diffusion models affects features at multiple scales - from fine textures to broader shapes. The U-Net's downsampling/upsampling path <i>(reducing/increasing the spatial dimensions of an image or feature map.)</i> processes information at various resolutions, making it effective for both tasks.\n",
    "- <b>Context integration with precision</b>: For medical segmentation, U-Net combines broad contextual information (Is this a liver? A brain?) with precise boundaries. For diffusion, it balances understanding the overall image structure while precisely identifying noise patterns at each pixel location.\n",
    "- <b>Transformation precision</b>: Both tasks require pixel-level precision in predicting output images. Whether defining the exact boundary of a tumor or determining exactly how much noise is at each pixel location, the U-Net's architecture is designed for this precise image-to-image mapping.\n",
    "- <b>Skip connections</b>: In a U-Net, a 'skip connection' directly connects a layer in the downsampling path to its corresponding layer in the upsampling path. For example, the features from the first downsampling layer are connected to the last upsampling layer, the second downsampling layer to the second-to-last upsampling layer, and so on. Skip connections solve a fundamental problem in deep networks: a) The downsampling path is great for capturing context but loses spatial precision b) The upsampling path needs to reconstruct spatial details but starts from compressed information\n",
    "  - In medical imaging: They help preserve fine boundary details that would be lost in downsampling\n",
    "  - In diffusion models: They help the model maintain the underlying image structure while focusing on noise prediction\n",
    "\n",
    "The fundamental similarity is that both tasks involve transforming one image into another while preserving important structural information - whether that's transforming a medical scan into a segmentation map or transforming a noisy image into a noise prediction (which then allows for denoising).\n",
    "\n",
    "Our implementation uses a simplified U-Net with:\n",
    "\n",
    "- A downsampling path that reduces spatial dimensions and increases feature channels\n",
    "- An upsampling path that increases spatial dimensions and decreases feature channels\n",
    "- Skip connections that concatenate features from the downsampling path to the upsampling path\n",
    "- Time embeddings that inform the network which diffusion timestep it's operating on\n",
    "\n",
    "### Time Embeddings\n",
    "\n",
    "A critical aspect of diffusion models is that the network needs to know which timestep it's denoising. We use sinusoidal position embeddings, similar to those in transformer architectures:\n",
    "\n",
    "```python\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "```\n",
    "\n",
    "These embeddings create unique representations for each timestep that the model can use to apply the appropriate amount of denoising.\n",
    "\n",
    "### Building Blocks\n",
    "\n",
    "The `Block` class forms the foundation of both the downsampling and upsampling paths:\n",
    "\n",
    "```python\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "```\n",
    "\n",
    "Each block:\n",
    "\n",
    "- Takes in a feature map and timestep embedding\n",
    "- Processes the feature map with convolutions and nonlinearities\n",
    "- Injects the timestep information via addition\n",
    "- Either downsamples (using strided convolution) or upsamples (using transposed convolution)\n",
    "\n",
    "### Complete U-Net Architecture\n",
    "\n",
    "The `SimpleUnet` class brings everything together:\n",
    "\n",
    "```python\n",
    "class SimpleUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "```\n",
    "\n",
    "The network:\n",
    "\n",
    "1. Takes an image and timestep as input\n",
    "2. Embeds the timestep using sinusoidal embeddings\n",
    "3. Processes the image through a series of downsampling blocks, reducing spatial dimensions and capturing increasingly abstract features\n",
    "4. Processes through upsampling blocks, gradually increasing spatial dimensions while incorporating features from the corresponding downsampling blocks via skip connections\n",
    "5. Produces an output with the same spatial dimensions and channels as the input image, representing the predicted noise\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "The model is trained to predict the noise that was added during the forward diffusion process. The loss function is simply the mean squared error between the predicted noise and the actual noise:\n",
    "\n",
    "$$L = \\mathbb{E}_{t,x_0,\\epsilon}[||\\epsilon - \\epsilon_\\theta(x_t, t)||^2]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\epsilon$ is the actual noise added\n",
    "- $\\epsilon_\\theta$ is the model's prediction of the noise\n",
    "- $x_t$ is the noisy image at timestep $t$\n",
    "\n",
    "By minimizing this loss, the model learns to estimate the noise component at any given timestep, which is the key to generating new images through the reverse diffusion process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(...,) + (None,) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture for M1 Pro compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=\"mps\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        image_channels = 4\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList(\n",
    "            [\n",
    "                Block(down_channels[i], down_channels[i + 1], time_emb_dim)\n",
    "                for i in range(len(down_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList(\n",
    "            [\n",
    "                Block(up_channels[i], up_channels[i + 1], time_emb_dim, up=True)\n",
    "                for i in range(len(up_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "        # Move model to appropriate device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Ensure inputs are on the correct device\n",
    "        x = x.to(self.device)\n",
    "        timestep = timestep.to(self.device)\n",
    "\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "def get_loss(model, x_0, t, device=\"mps\"):\n",
    "    \"\"\"\n",
    "    Calculates training loss for a batch of images at specified timesteps.\n",
    "    \"\"\"\n",
    "    x_0 = x_0.to(device)\n",
    "    t = t.to(device)\n",
    "\n",
    "    # Get noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "\n",
    "    # Get noisy image at timestep t\n",
    "    x_noisy, _ = forward_diffusion_sample(x_0, t, device)\n",
    "\n",
    "    # Get predicted noise from model\n",
    "    noise_pred = model(x_noisy, t)\n",
    "\n",
    "    # Calculate loss (mean squared error between actual and predicted noise)\n",
    "    loss = F.mse_loss(noise, noise_pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SimpleUnet(device=device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Num params: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "\"Loss\" refers to a mathematical measure of how well your diffusion model is performing.\n",
    "It quantifies the error or difference between what your model predicts and the actual target values.\n",
    "<br>For your diffusion model specifically:\n",
    "\n",
    "1. You start with a clean image (a WoW icon)\n",
    "2. You add a known amount of random noise to it\n",
    "3. You ask your model to predict what noise was added\n",
    "4. The \"loss\" measures how far off your model's prediction is from the actual noise that was added\n",
    "\n",
    "The F.l1_loss() function calculates the Mean Absolute Error (MAE), which is the average of the absolute differences between the predicted noise and the actual noise. A smaller loss value means your model is doing a better job at predicting the noise.<br>\n",
    "During training, your optimization algorithm (like Adam) uses this loss value to update the model's parameters, gradually improving its ability to predict the noise. The goal is to minimize this loss over many training iterations.<br>\n",
    "Once the model is trained to accurately predict the noise at each timestep, you can use it in reverse - starting with pure noise and gradually removing the predicted noise components to generate new WoW icons that never existed before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    \"\"\"\n",
    "    Calculates training loss for a batch of images at specified timesteps.\n",
    "    Handles channel dimension mismatch between noise and model output.\n",
    "    \"\"\"\n",
    "    # Create a noisy version of the original image at timestep t\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "\n",
    "    # Use the model to predict what noise was added\n",
    "    noise_pred = model(x_noisy, t)\n",
    "\n",
    "    # Adjust noise channels to match prediction channels if needed\n",
    "    if noise.shape[1] != noise_pred.shape[1]:\n",
    "        # Keep only the first 3 channels of noise to match noise_pred\n",
    "        noise = noise[:, : noise_pred.shape[1], :, :]\n",
    "\n",
    "    # Calculate the L1 loss (mean absolute error) between\n",
    "    # the actual noise and the predicted noise\n",
    "    return F.l1_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "### Key Implementation Details\n",
    "\n",
    "- These functions implement the sampling/generation process for creating new images with our diffusion model.\n",
    "\n",
    "- **Memory Management**: Without adding `@torch.no_grad()` we quickly run out of memory, because PyTorch tracks all the previous images for gradient calculation. When you use `@torch.no_grad()` as a decorator for your sampling functions, you're telling PyTorch not to track operations for automatic differentiation (gradients).\n",
    "\n",
    "- **Noise Schedule Consistency**: During the reverse process (image generation), we need to know exactly how much noise to remove at each step. For this to work properly, we need to use the same noise schedule that we used when adding noise. It's like following a recipe in reverse - you need to know the exact amounts that were added originally to remove them correctly.\n",
    "\n",
    "### Benefits of Pre-calculated Values\n",
    "\n",
    "The pre-calculated values ensure that:\n",
    "\n",
    "1. We're removing the right amount of noise at each step\n",
    "2. We're adding the right amount of new randomness when needed (yes, sometimes we add a small amount of new noise during generation)\n",
    "\n",
    "If we didn't use these pre-calculated values and instead made up new values during the reverse process, the math wouldn't work out correctly, and we'd get blurry or unrealistic images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    This function performs a single step of the reverse diffusion process,\n",
    "    taking a noisy image and making it slightly less noisy.\n",
    "    Calls the model to predict the noise in the image and returns\n",
    "    the denoised image. Applies noise to this image, if we are not in the last step yet.\n",
    "\n",
    "        1. Gets the appropriate noise parameters for the current timestep\n",
    "        2. Uses your trained model to predict what noise is present in the current noisy image\n",
    "        3. Uses that prediction to calculate a cleaner version of the image\n",
    "        4. If we're not at the final step, adds a small amount of new random noise (helps with sample quality)\n",
    "        5. Returns the slightly cleaner image\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    # Get model prediction (the model outputs only 3 channels)\n",
    "    model_output = model(x, t)\n",
    "\n",
    "    # Handle channel mismatch: pad model output to match input if needed\n",
    "    if model_output.shape[1] != x.shape[1]:\n",
    "        # Create a tensor of zeros with the same shape as x\n",
    "        padded_output = torch.zeros_like(x)\n",
    "        # Copy the available channels from model_output\n",
    "        padded_output[:, : model_output.shape[1]] = model_output\n",
    "        model_output = padded_output\n",
    "\n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model_output / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "\n",
    "    if t == 0:\n",
    "        # As pointed out by Luis Pereira (see YouTube comment)\n",
    "        # The t's are offset from the t's in the paper\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    \"\"\"\n",
    "    This function generates a completely new image by iteratively applying\n",
    "    the sampling process and visualizing the intermediate steps.\n",
    "        1. Starts with completely random noise (a new \"seed\" image)\n",
    "        2. Sets up a figure to display the generation process\n",
    "        3. Iterates through all timesteps in reverse order (from T down to 0)\n",
    "        4. At each step, calls sample_timestep to make the image a bit cleaner\n",
    "        5. Clamps the pixel values to keep them in the valid range\n",
    "        6. Periodically visualizes the current state to show the progressive denoising\n",
    "    \"\"\"\n",
    "    # Sample noise with 4 channels to match model's expected input\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn(\n",
    "        (1, 4, img_size, img_size), device=device\n",
    "    )  # Changed from 3 to 4 channels\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    plt.axis(\"off\")\n",
    "    num_images = 10\n",
    "    stepsize = int(T / num_images)\n",
    "\n",
    "    for i in range(0, T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        # Edit: This is to maintain the natural range of the distribution\n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i / stepsize) + 1)\n",
    "            # For display, we may need to use only the first 3 channels if showing RGB\n",
    "            display_img = img\n",
    "            if img.shape[1] > 3:\n",
    "                display_img = img[:, :3]  # Use only first 3 channels for display\n",
    "            show_tensor_image(display_img.detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Check for MPS (Metal Performance Shaders) for Mac with Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create a directory for checkpoints if it doesn't exist\n",
    "checkpoint_dir = \"diffusion_checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, filename=None):\n",
    "    \"\"\"\n",
    "    Save training checkpoint\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = f\"{checkpoint_dir}/checkpoint_epoch_{epoch}.pt\"\n",
    "\n",
    "    print(f\"Saving checkpoint to {filename}\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "        },\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "    # Also save a 'latest' checkpoint for easy resuming\n",
    "    latest_path = f\"{checkpoint_dir}/checkpoint_latest.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "        },\n",
    "        latest_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename=None):\n",
    "    \"\"\"\n",
    "    Load a training checkpoint\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = f\"{checkpoint_dir}/checkpoint_latest.pt\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"No checkpoint found at {filename}\")\n",
    "        return 0, 0\n",
    "\n",
    "    print(f\"Loading checkpoint from {filename}\")\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    return checkpoint[\"epoch\"], checkpoint[\"loss\"]\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training configuration\n",
    "epochs = 75\n",
    "checkpoint_frequency = 10  # Save a checkpoint every 10 epochs\n",
    "checkpoint_time_interval = 30 * 60  # Also save every 30 minutes\n",
    "resume_training = True  # Set to True to resume from latest checkpoint\n",
    "\n",
    "# Try to load previous checkpoint if resuming training\n",
    "start_epoch = 0\n",
    "if resume_training:\n",
    "    start_epoch, _ = load_checkpoint(model, optimizer)\n",
    "\n",
    "# Time of last checkpoint\n",
    "last_checkpoint_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    epoch_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "        loss = get_loss(model, batch, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        # Print progress\n",
    "        if step % 25 == 0:\n",
    "            print(f\"Step {step} of {len(dataloader)}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "        # Time-based checkpoint\n",
    "        current_time = time.time()\n",
    "        if current_time - last_checkpoint_time > checkpoint_time_interval:\n",
    "            save_checkpoint(\n",
    "                epoch,\n",
    "                model,\n",
    "                optimizer,\n",
    "                loss.item(),\n",
    "                f\"{checkpoint_dir}/checkpoint_time_{int(current_time)}.pt\",\n",
    "            )\n",
    "            last_checkpoint_time = current_time\n",
    "\n",
    "    # Calculate average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / batch_count\n",
    "    print(f\"Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "    # Visualization\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Generating sample image after epoch {epoch+1}\")\n",
    "        sample_plot_image()\n",
    "\n",
    "    # Epoch-based checkpoint\n",
    "    if (epoch + 1) % checkpoint_frequency == 0:\n",
    "        save_checkpoint(epoch + 1, model, optimizer, avg_epoch_loss)\n",
    "\n",
    "# Save final model\n",
    "save_checkpoint(\n",
    "    epochs, model, optimizer, avg_epoch_loss, f\"{checkpoint_dir}/model_final.pt\"\n",
    ")\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Save a standalone model for inference (just the weights, no optimizer state)\n",
    "torch.save(model.state_dict(), \"diffusion_model_final.pth\")\n",
    "print(\"Saved standalone model weights to diffusion_model_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "So .. Unfortunately my macbook isn't really optmised for this sort of model training 😅, in that, it would take me forever and I'm unable to practically optimise it (at least for this data set).\n",
    "I'm going to try the same approach but with the MNIST dataset, just to get an idea of how it works - Watch this space!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
