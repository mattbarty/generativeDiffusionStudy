{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Score-Based Diffusion Model with MNIST\n",
    "\n",
    "## Project Objectives & Overview\n",
    "\n",
    "In this notebook, I'll document my process of building a score-based diffusion model that can generate numbers from the MNIST dataset. Learn through doing!\n",
    "\n",
    "This implementation builds upon our previous simple <b>DDPM (\"Denoising Diffusion Probabilistic Model\")</b> approach, adapting it to the more mathematically elegant <b>score-based framework</b>. Like our DDPM, this model removes noise from random input images without any conditioning or optimizations. It's primarily a learning exercise to understand the fundamentals of score-based diffusion models.\n",
    "\n",
    "### The difference\n",
    "\n",
    "My previous DDPM implementation worked by:\n",
    "\n",
    "1. Gradually adding noise to images using a discrete Markov chain process\n",
    "2. Training a neural network to predict the noise that was added at each discrete timestep\n",
    "3. Reversing the process by iteratively removing predicted noise\n",
    "\n",
    "The score-based approach transforms this discrete process into a continuous one:\n",
    "\n",
    "1. Formulating the noise addition as a continuous stochastic differential equation (SDE)\n",
    "2. Training a neural network to predict the score function (gradient of log probability density) rather than the noise itself\n",
    "3. Using various numerical solvers (SDE/ODE solvers) for the reverse process\n",
    "4. This shift from discrete to continuous mathematics provides several significant advantages:\n",
    "\n",
    "### Why both?\n",
    "\n",
    "#### Visual Differences in Generated Samples\n",
    "\n",
    "In terms of final output quality, we may not see dramatic differences with MNIST specifically, because MNIST is relatively simple (black and white digits, minimal complexity), and both models can ultimately achieve good results on this dataset. This transition from discrete to continuous mathematics offers significant advantages while maintaining the core intuition of noise transformation.\n",
    "\n",
    "Subtle things we may notice:\n",
    "\n",
    "- Potentially sharper edges in some cases with score-based models\n",
    "- Better handling of ambiguous digits (like 4/9 or 3/8) where the distribution is multi-modal\n",
    "- More natural variations in generated samples (less artificial noise artifacts)\n",
    "\n",
    "Practically speaking, the advantages may be:\n",
    "\n",
    "1. Sampling Flexibility (most noticeable) - we'd be able to sample from any arbitrary point in the reverse process! This means:\n",
    "\n",
    "- You could start sampling from t=150.75 instead of just t=150\n",
    "- You could use adaptive step sizes (smaller steps for critical parts of digit formation, larger steps elsewhere)\n",
    "- You could compare different sampling trajectories from the same starting noise\n",
    "\n",
    "2. Significantly Faster Sampling\n",
    "   This is perhaps the biggest practical benefit - The previous DDPM requires 300 sequential steps for each sample. With score-based models using probability flow ODEs, you might generate comparable quality with 20-50 steps, which could speedup the generation time by 5-10x! Maybe I could demonstrate this with a chart showing generation time vs. sample quality - Let's see.\n",
    "\n",
    "3. Experimentation Options\n",
    "   You'd gain the ability to:\n",
    "\n",
    "- Try different numerical solvers and compare their outputs\n",
    "- Generate intermediate samples at arbitrary time points\n",
    "- Explore the trade-off between speed and quality more flexibly\n",
    "\n",
    "## DDPM vs. Score-Based Models Comparison\n",
    "\n",
    "| Aspect                     | DDPM (Discrete)                                    | Score-Based Models (Continuous)                                       | Benefits of Score-Based Approach                                                                                                                                                            | Limitations Overcome                                                                        |\n",
    "| -------------------------- | -------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Time Representation**    | Fixed numbered steps (t=0, t=1, t=2... t=300)      | Continuous time values (t can be any value between 0 and T)           | • More flexible sampling<br>• Can use adaptive step sizes<br>• Enables mathematical speedup techniques                                                                                      | Overcomes the rigid structure of fixed steps, allowing efficiency optimizations             |\n",
    "| **Mathematical Framework** | Sequence of explicit noise-adding formulas         | Stochastic Differential Equations (SDEs)                              | • More elegant mathematical foundation<br>• Connections to other math fields<br>• More options for solving reverse process<br>• Clearer theoretical properties                              | Replaces somewhat arbitrary discrete construction with well-studied mathematical properties |\n",
    "| **Learning Target**        | Predicts \"what noise was added\"                    | Predicts \"which direction increases data likelihood\" (score function) | • Directly relates to underlying data distribution<br>• Clearer conceptual framework<br>• Better connections with other generative approaches<br>• Better handling of complex distributions | Moves from indirect noise prediction to direct modeling of data distribution                |\n",
    "| **Sampling Flexibility**   | Fixed algorithm that must follow all reverse steps | Various numerical solvers (Euler-Maruyama, Predictor-Corrector, etc.) | • Speed vs. quality tradeoffs<br>• Advanced numerical methods<br>• Potentially much faster sampling (ODEs)<br>• Easier adaptation to different data types                                   | Addresses unnecessary slowness of DDPM sampling with adaptive approaches                    |\n",
    "| **Implementation Changes** | N/A                                                | U-Net architecture stays similar, but training and sampling change    | • Leverage existing architecture<br>• Main changes in math interpretation<br>• More flexibility without complete rewrite                                                                    | Maintains strengths while addressing weaknesses, particularly sampling speed                |\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "### Stochastic Differential Equations (SDEs)\n",
    "\n",
    "The forward process is modeled as a continuous SDE:\n",
    "\n",
    "dx = f(x,t)dt + g(t)dw\n",
    "\n",
    "Where:\n",
    "\n",
    "- f(x,t) is the drift term (deterministic component)\n",
    "- g(t) is the diffusion coefficient\n",
    "- dw is a standard Wiener process (Brownian motion)\n",
    "\n",
    "This formulation precisely controls how data transforms into noise through continuous time rather than discrete steps.\n",
    "\n",
    "### The Score Function\n",
    "\n",
    "The core innovation is predicting the score function - the gradient of the log probability density:\n",
    "\n",
    "∇x log p(x)\n",
    "\n",
    "This function points toward regions of higher data probability density, directly guiding the model in generating realistic images.\n",
    "\n",
    "## Implementation Requirements\n",
    "\n",
    "To transform our DDPM into a score-based model, I'll need to make these key modifications:\n",
    "\n",
    "1. **SDE Formulation**: Replace discrete beta schedule with continuous SDE coefficients\n",
    "2. **Score Matching Loss**: Implement denoising score matching rather than noise prediction\n",
    "3. **Numerical Solvers**: Create SDE/ODE solvers for the reverse process\n",
    "4. **Network Adaptation**: Adjust the U-Net to predict the score instead of noise\n",
    "5. **Continuous Time**: Update time embeddings and sampling to work with continuous values\n",
    "\n",
    "These changes will enable:\n",
    "\n",
    "- Starting generation from any arbitrary timepoint\n",
    "- Using adaptive step sizes for efficiency\n",
    "- Leveraging probability flow ODEs for faster sampling\n",
    "- Applying various numerical methods with different quality/speed tradeoffs\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "- **Name**: MNIST Database of handwritten digits\n",
    "- **Source**: MNIST Dataset\n",
    "- **Format**: 28×28 grayscale images of handwritten digits (0-9)\n",
    "\n",
    "### Implementation Notes\n",
    "\n",
    "This implementation adapts the score-based diffusion model architecture to work with:\n",
    "\n",
    "- TGA format images (using PIL for loading)\n",
    "- Apple Silicon hardware (ensuring MPS device compatibility)\n",
    "\n",
    "### Credits and References\n",
    "\n",
    "This implementation builds upon our previous DDPM work and takes inspiration from:\n",
    "\n",
    "- Song, Y., et al. (2021). \"Score-Based Generative Modeling through Stochastic Differential Equations\"\n",
    "- Yang, L., & Zhang, Z. (2022). \"Diffusion Models: A Comprehensive Survey of Methods and Applications\"\n",
    "- Karras, T., et al. (2022). \"Elucidating the Design Space of Diffusion-Based Generative Models\"\n",
    "\n",
    "The original DDPM work referenced:\n",
    "\n",
    "- \"Denoising Diffusion Probabilistic Models\" (Ho et al., 2020)\n",
    "- \"Improved Denoising Diffusion Probabilistic Models\" (Nichol & Dhariwal, 2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load MNIST dataset\n",
    "\n",
    "# Define transformations for MNIST dataset\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert image to tensor (0,1)\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1,1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "data = datasets.MNIST(\n",
    "    root=data_dir, train=True, transform=data_transforms, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read Image & Label from given file path\n",
    "def read_idx_images(file_path):\n",
    "    \"\"\"Reads an IDX image file and returns a tensor of shape (N, 28, 28)\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        f.read(4)  # Skip magic number\n",
    "        num_images = int.from_bytes(f.read(4), \"big\")\n",
    "        rows = int.from_bytes(f.read(4), \"big\")\n",
    "        cols = int.from_bytes(f.read(4), \"big\")\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "        return torch.tensor(data, dtype=torch.float32)  # Convert to float\n",
    "\n",
    "\n",
    "def read_idx_labels(file_path):\n",
    "    \"\"\"Reads an IDX label file and returns a tensor of shape (N,)\"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        f.read(4)  # Skip magic number\n",
    "        num_labels = int.from_bytes(f.read(4), \"big\")\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return torch.tensor(data, dtype=torch.long)  # Convert to long tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming read_idx_images loads a tensor of shape (N, 28, 28)\n",
    "dataset_root_path = \"./data/MNIST/raw/t10k-images-idx3-ubyte\"\n",
    "dataset = read_idx_images(dataset_root_path)  # Should return a (N, 28, 28) tensor\n",
    "\n",
    "# Select 15 random indices\n",
    "num_samples = 15\n",
    "random_indices = torch.randint(0, len(dataset), (num_samples,))  # Random indices\n",
    "dataset_sample = dataset[random_indices]  # Get the images\n",
    "\n",
    "# Display the images\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, image in enumerate(dataset_sample):\n",
    "    plt.subplot(3, 5, i + 1)  # Use 3x5 grid\n",
    "    plt.imshow(image.numpy(), cmap=\"gray\")  # Convert tensor to NumPy\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Random Samples from Dataset\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Schedule and Forward Diffusion\n",
    "\n",
    "The forward diffusion process gradually adds controlled amounts of noise to your images over time. This process uses several key elements:\n",
    "\n",
    "### Noise Schedule\n",
    "\n",
    "Our diffusion model uses a linear noise schedule that starts with minimal noise (0.0001) and gradually increases to more substantial noise (0.02) over 300 timesteps. This controlled progression ensures that information is destroyed at a predictable rate.\n",
    "\n",
    "The start value (0.0001) is very small because:\n",
    "\n",
    "- You want to begin with minimal distortion to the original image\n",
    "- It allows the model to learn the subtlest noise patterns first\n",
    "- Too large an initial value would destroy fine details too quickly\n",
    "\n",
    "The end value (0.02) is moderate because:\n",
    "\n",
    "- It's large enough to eventually transform the image into complete noise\n",
    "- But not so large that the transition becomes too abrupt\n",
    "- This gradual approach helps the model learn the reverse process more effectively\n",
    "\n",
    "The 300 timesteps are chosen because:\n",
    "\n",
    "- More timesteps (1000+) would give slightly better quality but significantly slower training and sampling\n",
    "- Fewer timesteps (50-100) would be faster but might produce lower quality results\n",
    "- 300 represents a practical compromise for the MNIST dataset\n",
    "\n",
    "### Adding Noise to Images\n",
    "\n",
    "The heart of the forward diffusion process combines two components:\n",
    "\n",
    "1. A portion of the original image that diminishes over time\n",
    "2. A portion of random noise that increases over time\n",
    "\n",
    "As we move through the timesteps, less of the original image remains visible and more random noise dominates. This creates a smooth transition from clean images to pure noise.\n",
    "\n",
    "### Pre-calculated Parameters\n",
    "\n",
    "For efficiency, we pre-calculate all the mathematical constants needed for both the forward and reverse diffusion processes. These constants control exactly how much of the original image and how much noise should be present at each timestep.\n",
    "\n",
    "When training the model, we randomly sample timesteps and teach the network to predict what noise was added. Later, we can reverse this process - starting with pure noise and gradually removing it to generate new, realistic MNIST digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    \"\"\"\n",
    "    Creates a linear schedule for the noise variance (β) at each timestep:\n",
    "\n",
    "    timesteps: the number of timesteps\n",
    "    start: the starting value of the schedule\n",
    "    end: the ending value of the schedule\n",
    "    \"\"\"\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "\n",
    "    vals: the list of values to get the index from\n",
    "    t: the timestep\n",
    "    x_shape: the shape of the input\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"mps\"):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "\n",
    "    x_0: the original image\n",
    "    t: the timestep\n",
    "    device: the device to run the computation on\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(\n",
    "        device\n",
    "    ) + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Data Loading Pipeline\n",
    "\n",
    "This section sets up the data loading pipeline for MNIST dataset. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Check for MPS availability\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "IMG_SIZE = 28  # MNIST images are 28x28\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Define transformations for MNIST dataset\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Convert image to tensor (0,1)\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # Normalize to [-1,1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Configure DataLoader\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,  # Avoid pickle issues with multiple workers\n",
    ")\n",
    "\n",
    "\n",
    "# Function to show a single tensor image\n",
    "def show_tensor_image(image):\n",
    "    \"\"\"\n",
    "    Display a tensor as an image\n",
    "    \"\"\"\n",
    "    # Remove batch dimension\n",
    "    if len(image.shape) == 4:  # [B, C, H, W]\n",
    "        image = image.squeeze(0)  # → [C, H, W]\n",
    "\n",
    "    # For grayscale (MNIST) images\n",
    "    if image.shape[0] == 1:  # [1, H, W]\n",
    "        image = image.squeeze(0)  # → [H, W]\n",
    "\n",
    "    # Scale from [-1, 1] to [0, 1]\n",
    "    image = (image + 1) / 2\n",
    "\n",
    "    # Convert to numpy for display\n",
    "    image_np = image.numpy()\n",
    "\n",
    "    # Display as grayscale\n",
    "    plt.imshow(image_np, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# Function to move batch to device efficiently\n",
    "def prepare_batch(batch, target_device=device):\n",
    "    return batch[0].to(target_device)  # Only move image tensors (ignore labels)\n",
    "\n",
    "\n",
    "# Test: Display a sample image from the dataset\n",
    "sample_image, _ = data[0]  # Get first image\n",
    "show_tensor_image(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization displays the forward diffusion process applied to MNIST digits. Each row shows a different randomly selected digit from the dataset, while columns represent progressive timesteps in the diffusion process.<br><br>\n",
    "Moving from left to right along each row, you see how the diffusion model gradually adds more noise to the original clean images. The leftmost column shows the original, unmodified MNIST digits in their clean state. As you move right, each subsequent image shows the same digit with increasing amounts of random noise applied according to a carefully controlled schedule (T=300).<br><br>\n",
    "By the rightmost column, the digits have been almost completely transformed into pure noise, with little to no recognizable features remaining from the original images. This visual demonstration illustrates how the forward diffusion process systematically destroys information in the original images over time.<br><br>\n",
    "Understanding this forward process is crucial because training a diffusion model involves teaching a neural network to reverse this exact process - starting from pure noise and progressively removing it to generate new, realistic handwritten digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_diffusion(\n",
    "    dataloader, num_images=3, num_steps=7, T=300, device=\"mps\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Display a grid of randomly selected images with the diffusion process applied.\n",
    "    Each row shows one icon progressively diffused with noise from left to right.\n",
    "    The first column shows the original, unmodified icon.\n",
    "\n",
    "    Args:\n",
    "        dataloader: PyTorch dataloader containing the dataset\n",
    "        num_images: Number of different images to display (rows)\n",
    "        num_steps: Number of diffusion steps to display (not including original)\n",
    "        T: Total number of timesteps in the diffusion process\n",
    "        device: Device to run computations on\n",
    "    \"\"\"\n",
    "    # Get a batch of images\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    # Move batch to device\n",
    "    batch = prepare_batch(batch, device)\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(\n",
    "        num_images, num_steps + 1, figsize=((num_steps + 1) * 2, num_images * 2)\n",
    "    )\n",
    "\n",
    "    # Get random indices from the batch\n",
    "    batch_size = batch.shape[0]\n",
    "    indices = np.random.choice(batch_size, size=num_images, replace=False)\n",
    "\n",
    "    # Calculate the step size between diffusion steps\n",
    "    stepsize = int(T / num_steps)\n",
    "\n",
    "    # Plot each selected image with different noise levels\n",
    "    for row, idx in enumerate(indices):\n",
    "        # Get the original image\n",
    "        original_img = batch[idx].unsqueeze(0)  # Add batch dimension\n",
    "        original_display = original_img.squeeze(0)  # Ensure shape is (H, W)\n",
    "\n",
    "        # First, display the original image with no noise\n",
    "        ax = axes[row, 0]  # First column\n",
    "        ax.imshow(original_display.squeeze(0).cpu().numpy(), cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        if row == 0:\n",
    "            ax.set_title(\"Original\", fontsize=8)\n",
    "\n",
    "        # Apply different levels of noise for the remaining columns\n",
    "        for col, timestep in enumerate(range(0, T, stepsize)):\n",
    "            if col >= num_steps:\n",
    "                break\n",
    "\n",
    "            ax = axes[row, col + 1]  # Remaining columns\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "            # Apply forward diffusion with current timestep\n",
    "            t = torch.tensor([timestep], device=device).type(torch.int64)\n",
    "            noisy_img, _ = forward_diffusion_sample(original_img, t, device=device)\n",
    "            noisy_img = noisy_img.squeeze().cpu().numpy()  # Ensure shape is (H, W)\n",
    "\n",
    "            # Display the image\n",
    "            ax.imshow(noisy_img, cmap=\"gray\")\n",
    "\n",
    "            # Add timestep as title for the first row\n",
    "            if row == 0:\n",
    "                ax.set_title(f\"t={timestep}\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "display_images_with_diffusion(dataloader, num_images=3, num_steps=7, T=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reverse Diffusion Process and U-Net Architecture\n",
    "\n",
    "While the forward diffusion process gradually adds noise to images, the reverse diffusion process is where the magic bits happens - <br>It's how we generate new images by progressively removing noise. This is accomplished using a neural network (in our case, a U-Net) that predicts the noise component at each timestep.\n",
    "\n",
    "### The Reverse Diffusion Equation\n",
    "\n",
    "The diffusion model learns to reverse the forward process by predicting the noise ε that was added at each timestep. The mathematical equation for the reverse process is:\n",
    "\n",
    "$$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $p_\\theta$ is our model's reverse distribution\n",
    "- $\\mu_\\theta$ is the predicted mean\n",
    "- $\\Sigma_\\theta$ is the predicted variance\n",
    "\n",
    "Instead of trying to learn how to generate images directly (which is very hard), the neural network (U-Net) is trained on a simpler task: \"Given an image with noise at step t, can you predict what noise was added?\". Once the network is trained to predict noise accurately, we can use it to generate brand new images by starting with pure random noise and then iteratively \"removing\" the predicted noise.\n",
    "\n",
    "### U-Net Architecture Overview\n",
    "\n",
    "A U-Net is a specific type of neural network architecture that was originally developed for biomedical image segmentation but has become widely used across many image processing tasks, including diffusion models.\n",
    "The mechanism that makes U-Net effective for both medical image segmentation and diffusion models comes down to some key architectural properties that transfer well between these seemingly different tasks:\n",
    "\n",
    "- <b>Preserving spatial relationships</b>: In medical imaging, U-Net excels at identifying boundaries and structures while maintaining their spatial relationships. In diffusion models, this same ability helps preserve the underlying structure of an image while noise is being predicted or removed.\n",
    "- <b>Multi-scale feature processing</b>: Medical images often contain features at different scales (from tiny cell structures to larger organs). Similarly, noise in diffusion models affects features at multiple scales - from fine textures to broader shapes. The U-Net's downsampling/upsampling path <i>(reducing/increasing the spatial dimensions of an image or feature map.)</i> processes information at various resolutions, making it effective for both tasks.\n",
    "- <b>Context integration with precision</b>: For medical segmentation, U-Net combines broad contextual information (Is this a liver? A brain?) with precise boundaries. For diffusion, it balances understanding the overall image structure while precisely identifying noise patterns at each pixel location.\n",
    "- <b>Transformation precision</b>: Both tasks require pixel-level precision in predicting output images. Whether defining the exact boundary of a tumor or determining exactly how much noise is at each pixel location, the U-Net's architecture is designed for this precise image-to-image mapping.\n",
    "- <b>Skip connections</b>: In a U-Net, a 'skip connection' directly connects a layer in the downsampling path to its corresponding layer in the upsampling path. For example, the features from the first downsampling layer are connected to the last upsampling layer, the second downsampling layer to the second-to-last upsampling layer, and so on. Skip connections solve a fundamental problem in deep networks: a) The downsampling path is great for capturing context but loses spatial precision b) The upsampling path needs to reconstruct spatial details but starts from compressed information\n",
    "  - In medical imaging: They help preserve fine boundary details that would be lost in downsampling\n",
    "  - In diffusion models: They help the model maintain the underlying image structure while focusing on noise prediction\n",
    "\n",
    "The fundamental similarity is that both tasks involve transforming one image into another while preserving important structural information - whether that's transforming a medical scan into a segmentation map or transforming a noisy image into a noise prediction (which then allows for denoising).\n",
    "\n",
    "Our implementation uses a simplified U-Net with:\n",
    "\n",
    "- A downsampling path that reduces spatial dimensions and increases feature channels\n",
    "- An upsampling path that increases spatial dimensions and decreases feature channels\n",
    "- Skip connections that concatenate features from the downsampling path to the upsampling path\n",
    "- Time embeddings that inform the network which diffusion timestep it's operating on\n",
    "\n",
    "### Time Embeddings\n",
    "\n",
    "A critical aspect of diffusion models is that the network needs to know which timestep it's denoising. We use sinusoidal position embeddings, similar to those in transformer architectures:\n",
    "\n",
    "```python\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "```\n",
    "\n",
    "These embeddings create unique representations for each timestep that the model can use to apply the appropriate amount of denoising.\n",
    "\n",
    "### Building Blocks\n",
    "\n",
    "The `Block` class forms the foundation of both the downsampling and upsampling paths:\n",
    "\n",
    "```python\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "```\n",
    "\n",
    "Each block:\n",
    "\n",
    "- Takes in a feature map and timestep embedding\n",
    "- Processes the feature map with convolutions and nonlinearities\n",
    "- Injects the timestep information via addition\n",
    "- Either downsamples (using strided convolution) or upsamples (using transposed convolution)\n",
    "\n",
    "### Complete U-Net Architecture\n",
    "\n",
    "The `SimpleUnet` class brings everything together:\n",
    "\n",
    "```python\n",
    "class SimpleUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "```\n",
    "\n",
    "The network:\n",
    "\n",
    "1. Takes an image and timestep as input\n",
    "2. Embeds the timestep using sinusoidal embeddings\n",
    "3. Processes the image through a series of downsampling blocks, reducing spatial dimensions and capturing increasingly abstract features\n",
    "4. Processes through upsampling blocks, gradually increasing spatial dimensions while incorporating features from the corresponding downsampling blocks via skip connections\n",
    "5. Produces an output with the same spatial dimensions and channels as the input image, representing the predicted noise\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "The model is trained to predict the noise that was added during the forward diffusion process. The loss function is simply the mean squared error between the predicted noise and the actual noise:\n",
    "\n",
    "$$L = \\mathbb{E}_{t,x_0,\\epsilon}[||\\epsilon - \\epsilon_\\theta(x_t, t)||^2]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\epsilon$ is the actual noise added\n",
    "- $\\epsilon_\\theta$ is the model's prediction of the noise\n",
    "- $x_t$ is the noisy image at timestep $t$\n",
    "\n",
    "By minimizing this loss, the model learns to estimate the noise component at any given timestep, which is the key to generating new images through the reverse diffusion process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        # Replace BatchNorm with GroupNorm which works with any batch size\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)  # 8 groups is a good default\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # First Conv\n",
    "        h = self.norm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(...,) + (None,) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.norm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified variant of the Unet architecture for M1 Pro compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=\"mps\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        image_channels = 1\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 1\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList(\n",
    "            [\n",
    "                Block(down_channels[i], down_channels[i + 1], time_emb_dim)\n",
    "                for i in range(len(down_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList(\n",
    "            [\n",
    "                Block(up_channels[i], up_channels[i + 1], time_emb_dim, up=True)\n",
    "                for i in range(len(up_channels) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "        # Move model to appropriate device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Ensure inputs are on the correct device\n",
    "        x = x.to(self.device)\n",
    "        timestep = timestep.to(self.device)\n",
    "\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "\n",
    "            # Ensure spatial dimensions match before concatenation\n",
    "            if x.shape[2:] != residual_x.shape[2:]:\n",
    "                # Resize x to match residual_x's spatial dimensions\n",
    "                x = nn.functional.interpolate(\n",
    "                    x, size=residual_x.shape[2:], mode=\"bilinear\", align_corners=False\n",
    "                )\n",
    "\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "def get_loss(model, x_0, t, device=\"mps\"):\n",
    "    \"\"\"\n",
    "    Calculates training loss for a batch of images at specified timesteps.\n",
    "    \"\"\"\n",
    "    x_0 = x_0.to(device)\n",
    "    t = t.to(device)\n",
    "\n",
    "    # Get noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "\n",
    "    # Get noisy image at timestep t\n",
    "    x_noisy, _ = forward_diffusion_sample(x_0, t, device)\n",
    "\n",
    "    # Get predicted noise from model\n",
    "    noise_pred = model(x_noisy, t)\n",
    "\n",
    "    # Calculate loss (mean squared error between actual and predicted noise)\n",
    "    loss = F.mse_loss(noise, noise_pred)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SimpleUnet(device=device)\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Num params: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "\"Loss\" refers to a mathematical measure of how well your diffusion model is performing.\n",
    "It quantifies the error or difference between what your model predicts and the actual target values.\n",
    "<br>For your diffusion model specifically:\n",
    "\n",
    "1. You start with a clean image\n",
    "2. You add a known amount of random noise to it\n",
    "3. You ask your model to predict what noise was added\n",
    "4. The \"loss\" measures how far off your model's prediction is from the actual noise that was added\n",
    "\n",
    "The F.l1_loss() function calculates the Mean Absolute Error (MAE), which is the average of the absolute differences between the predicted noise and the actual noise. A smaller loss value means your model is doing a better job at predicting the noise.<br>\n",
    "During training, your optimization algorithm (like Adam) uses this loss value to update the model's parameters, gradually improving its ability to predict the noise. The goal is to minimize this loss over many training iterations.<br>\n",
    "Once the model is trained to accurately predict the noise at each timestep, you can use it in reverse - starting with pure noise and gradually removing the predicted noise components to generate new image (a variant of the MNIST dataset that is characteristically a number, but not in the training dataset.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(model, x_0, t):\n",
    "    \"\"\"\n",
    "    Calculates training loss for a batch of images at specified timesteps.\n",
    "    Handles channel dimension mismatch between noise and model output.\n",
    "    \"\"\"\n",
    "    # Create a noisy version of the original image at timestep t\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "\n",
    "    # Use the model to predict what noise was added\n",
    "    noise_pred = model(x_noisy, t)\n",
    "\n",
    "    # Adjust noise channels to match prediction channels if needed\n",
    "    if noise.shape[1] != noise_pred.shape[1]:\n",
    "        # Keep only the first 3 channels of noise to match noise_pred\n",
    "        noise = noise[:, : noise_pred.shape[1], :, :]\n",
    "\n",
    "    # Calculate the L1 loss (mean absolute error) between\n",
    "    # the actual noise and the predicted noise\n",
    "    return F.l1_loss(noise, noise_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "### Key Implementation Details\n",
    "\n",
    "- These functions implement the sampling/generation process for creating new images with our diffusion model.\n",
    "\n",
    "- **Memory Management**: Without adding `@torch.no_grad()` we quickly run out of memory, because PyTorch tracks all the previous images for gradient calculation. When you use `@torch.no_grad()` as a decorator for your sampling functions, you're telling PyTorch not to track operations for automatic differentiation (gradients).\n",
    "\n",
    "- **Noise Schedule Consistency**: During the reverse process (image generation), we need to know exactly how much noise to remove at each step. For this to work properly, we need to use the same noise schedule that we used when adding noise. It's like following a recipe in reverse - you need to know the exact amounts that were added originally to remove them correctly.\n",
    "\n",
    "### Benefits of Pre-calculated Values\n",
    "\n",
    "The pre-calculated values ensure that:\n",
    "\n",
    "1. We're removing the right amount of noise at each step\n",
    "2. We're adding the right amount of new randomness when needed (yes, sometimes we add a small amount of new noise during generation)\n",
    "\n",
    "If we didn't use these pre-calculated values and instead made up new values during the reverse process, the math wouldn't work out correctly, and we'd get blurry or unrealistic images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    \"\"\"\n",
    "    This function performs a single step of the reverse diffusion process,\n",
    "    taking a noisy image and making it slightly less noisy.\n",
    "    Calls the model to predict the noise in the image and returns\n",
    "    the denoised image. Applies noise to this image, if we are not in the last step yet.\n",
    "\n",
    "        1. Gets the appropriate noise parameters for the current timestep\n",
    "        2. Uses your trained model to predict what noise is present in the current noisy image\n",
    "        3. Uses that prediction to calculate a cleaner version of the image\n",
    "        4. If we're not at the final step, adds a small amount of new random noise (helps with sample quality)\n",
    "        5. Returns the slightly cleaner image\n",
    "    \"\"\"\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    # Get model prediction (the model outputs only 3 channels)\n",
    "    model_output = model(x, t)\n",
    "\n",
    "    # Handle channel mismatch: pad model output to match input if needed\n",
    "    if model_output.shape[1] != x.shape[1]:\n",
    "        # Create a tensor of zeros with the same shape as x\n",
    "        padded_output = torch.zeros_like(x)\n",
    "        # Copy the available channels from model_output\n",
    "        padded_output[:, : model_output.shape[1]] = model_output\n",
    "        model_output = padded_output\n",
    "\n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model_output / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "\n",
    "    if t == 0:\n",
    "        # As pointed out by Luis Pereira (see YouTube comment)\n",
    "        # The t's are offset from the t's in the paper\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    \"\"\"\n",
    "    This function generates a completely new image by iteratively applying\n",
    "    the sampling process and visualizing the intermediate steps.\n",
    "        1. Starts with completely random noise (a new \"seed\" image)\n",
    "        2. Sets up a figure to display the generation process\n",
    "        3. Iterates through all timesteps in reverse order (from T down to 0)\n",
    "        4. At each step, calls sample_timestep to make the image a bit cleaner\n",
    "        5. Clamps the pixel values to keep them in the valid range\n",
    "        6. Periodically visualizes the current state to show the progressive denoising\n",
    "    \"\"\"\n",
    "    # Sample noise with 4 channels to match model's expected input\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn(\n",
    "        (1, 1, img_size, img_size), device=device\n",
    "    )  # Changed from 3 to 4 channels\n",
    "\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    plt.axis(\"off\")\n",
    "    num_images = 10\n",
    "    stepsize = int(T / num_images)\n",
    "\n",
    "    for i in range(0, T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        # Edit: This is to maintain the natural range of the distribution\n",
    "        img = torch.clamp(img, -1.0, 1.0)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i / stepsize) + 1)\n",
    "            # For MNIST, we only have 1 channel, so no need to select channels for display\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Check for MPS (Metal Performance Shaders) for Mac with Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 75  # Try more!\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch} of {epochs}\")\n",
    "\n",
    "    # iterate over the dataloader\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # if step % 25 == 0:\n",
    "        #     print(f\"Step {step} of {len(dataloader)}\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img_tensor = batch[0]\n",
    "        label_tensor = batch[1]\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "        loss = get_loss(model, img_tensor, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 15 == 0 and step == 0:\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "            sample_plot_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"MNIST_diffusion_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT WORKS !! 🎉\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_plot_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import os\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def create_diffusion_video(\n",
    "    filename=\"diffusion_video.mp4\", fps=30, dpi=150, show_every=1, grid=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a video showing the diffusion process from noise to coherent image.\n",
    "    Optionally arranges multiple diffusion processes in a grid layout.\n",
    "\n",
    "    Args:\n",
    "        filename: Output video filename (should end with .mp4)\n",
    "        fps: Frames per second in the output video\n",
    "        dpi: Resolution of the output video\n",
    "        show_every: Only include every Nth step (to reduce video size)\n",
    "        grid: Optional tuple (rows, cols) to create a grid of diffusion processes.\n",
    "              If None, creates a single video.\n",
    "\n",
    "    Returns:\n",
    "        Path to the saved video file\n",
    "    \"\"\"\n",
    "    # Determine if we're making a grid or single video\n",
    "    if grid is None:\n",
    "        rows, cols = 1, 1\n",
    "    else:\n",
    "        rows, cols = grid\n",
    "\n",
    "    # Set up figure with the right size and layout\n",
    "    if rows == 1 and cols == 1:\n",
    "        fig, axes = plt.subplots(figsize=(6, 7))  # Extra height for the title\n",
    "        axes = np.array([[axes]])  # Make it a 2D array for consistent indexing\n",
    "    else:\n",
    "        fig, axes = plt.subplots(\n",
    "            rows, cols, figsize=(cols * 3, rows * 3 + 1)\n",
    "        )  # Extra height for the title\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, cols)\n",
    "        elif cols == 1:\n",
    "            axes = axes.reshape(rows, 1)\n",
    "\n",
    "    # Remove ticks from all subplots\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            axes[r, c].set_xticks([])\n",
    "            axes[r, c].set_yticks([])\n",
    "\n",
    "    # Add adequate padding to make room for the title\n",
    "    plt.subplots_adjust(top=0.85)  # Move subplots down to make room for title\n",
    "\n",
    "    # Start with random noise for each grid position\n",
    "    img_size = IMG_SIZE\n",
    "    grid_images = []\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            # Create a random seed image\n",
    "            img = torch.randn((1, 1, img_size, img_size), device=device)\n",
    "            grid_images.append(img)\n",
    "\n",
    "    # Store all frames for the animation\n",
    "    all_frames = []\n",
    "\n",
    "    print(\n",
    "        f\"Generating {T//show_every} frames for {'single image' if grid is None else f'{rows}x{cols} grid'}...\"\n",
    "    )\n",
    "\n",
    "    # Run the reverse diffusion process for all images\n",
    "    for i in range(T - 1, -1, -1):\n",
    "        # Only capture frames at specified intervals\n",
    "        if i % show_every == 0:\n",
    "            # Create a frame showing the current state of all images\n",
    "            frame_data = []\n",
    "\n",
    "            for idx, img in enumerate(grid_images):\n",
    "                r, c = divmod(idx, cols)\n",
    "\n",
    "                # Get current state of this image\n",
    "                display_img = img[0].detach().cpu()\n",
    "                if display_img.shape[0] == 1:  # Handle grayscale\n",
    "                    display_img = display_img.squeeze(0)\n",
    "\n",
    "                # Convert to numpy and scale to [0,1]\n",
    "                display_img = (display_img + 1) / 2\n",
    "                display_img = display_img.numpy()\n",
    "\n",
    "                # Store for this frame\n",
    "                frame_data.append((r, c, display_img))\n",
    "\n",
    "            # Add the step number\n",
    "            all_frames.append((frame_data, f\"Diffusion Process - Step {T-i}/{T}\"))\n",
    "\n",
    "        # Now perform denoising step for all images\n",
    "        for idx in range(len(grid_images)):\n",
    "            t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "            grid_images[idx] = sample_timestep(grid_images[idx], t)\n",
    "            grid_images[idx] = torch.clamp(grid_images[idx], -1.0, 1.0)\n",
    "\n",
    "        # Show progress\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed step {T-i}/{T}\")\n",
    "\n",
    "    # Make sure to include the final state\n",
    "    final_frame_data = []\n",
    "    for idx, img in enumerate(grid_images):\n",
    "        r, c = divmod(idx, cols)\n",
    "        display_img = img[0].detach().cpu()\n",
    "        if display_img.shape[0] == 1:\n",
    "            display_img = display_img.squeeze(0)\n",
    "        display_img = (display_img + 1) / 2\n",
    "        display_img = display_img.numpy()\n",
    "        final_frame_data.append((r, c, display_img))\n",
    "\n",
    "    all_frames.append((final_frame_data, \"Final Result\"))\n",
    "\n",
    "    print(f\"Captured {len(all_frames)} frames. Creating animation...\")\n",
    "\n",
    "    # Set up the animation\n",
    "    def init():\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                axes[r, c].clear()\n",
    "                axes[r, c].set_xticks([])\n",
    "                axes[r, c].set_yticks([])\n",
    "        # Remove any existing title\n",
    "        if hasattr(fig, \"_suptitle\") and fig._suptitle is not None:\n",
    "            fig._suptitle.set_text(\"\")\n",
    "        return []\n",
    "\n",
    "    def animate(i):\n",
    "        grid_data, title = all_frames[i]\n",
    "\n",
    "        # Update all grid positions\n",
    "        for r, c, img_data in grid_data:\n",
    "            axes[r, c].clear()\n",
    "            axes[r, c].imshow(img_data, cmap=\"gray\")\n",
    "            axes[r, c].set_xticks([])\n",
    "            axes[r, c].set_yticks([])\n",
    "\n",
    "            # Add individual titles if it's a grid\n",
    "            if rows > 1 or cols > 1:\n",
    "                idx = r * cols + c\n",
    "                axes[r, c].set_title(f\"Sample {idx+1}\")\n",
    "\n",
    "        # Add a global title that's clearly visible\n",
    "        fig.suptitle(title, fontsize=16, y=0.98)\n",
    "\n",
    "        return []\n",
    "\n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        init_func=init,\n",
    "        frames=len(all_frames),\n",
    "        interval=1000 / fps,\n",
    "        blit=False,\n",
    "    )  # blit=False ensures title updates properly\n",
    "\n",
    "    # Save animation\n",
    "    print(f\"Saving video to {filename}...\")\n",
    "    writer = animation.FFMpegWriter(\n",
    "        fps=fps, metadata=dict(artist=\"Diffusion Model\"), bitrate=3600\n",
    "    )\n",
    "    anim.save(filename, writer=writer, dpi=dpi)\n",
    "\n",
    "    plt.close()\n",
    "    print(f\"Video saved successfully to {filename}\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Alternative version that generates frames instead of animation\n",
    "@torch.no_grad()\n",
    "def create_diffusion_frames(output_dir=\"diffusion_frames\", show_every=1, grid=None):\n",
    "    \"\"\"\n",
    "    Saves individual frames of the diffusion process(es).\n",
    "\n",
    "    Args:\n",
    "        output_dir: Directory to save the frames\n",
    "        show_every: Only save every Nth frame\n",
    "        grid: Optional tuple (rows, cols) to create a grid of diffusion processes.\n",
    "              If None, creates frames for a single diffusion process.\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Determine if we're making a grid or single video\n",
    "    if grid is None:\n",
    "        rows, cols = 1, 1\n",
    "    else:\n",
    "        rows, cols = grid\n",
    "\n",
    "    # Start with random noise for each grid position\n",
    "    img_size = IMG_SIZE\n",
    "    grid_images = []\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            img = torch.randn((1, 1, img_size, img_size), device=device)\n",
    "            grid_images.append(img)\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Run the reverse diffusion process\n",
    "    for i in range(T - 1, -1, -1):\n",
    "        if i % show_every == 0:\n",
    "            # Create a figure with appropriate layout\n",
    "            if rows == 1 and cols == 1:\n",
    "                fig, axes = plt.subplots(figsize=(6, 7))\n",
    "                axes = np.array([[axes]])\n",
    "            else:\n",
    "                fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3 + 1))\n",
    "                if rows == 1:\n",
    "                    axes = axes.reshape(1, cols)\n",
    "                elif cols == 1:\n",
    "                    axes = axes.reshape(rows, 1)\n",
    "\n",
    "            plt.subplots_adjust(top=0.85)  # Make room for the title\n",
    "\n",
    "            # Update each subplot with current image state\n",
    "            for idx, img in enumerate(grid_images):\n",
    "                r, c = divmod(idx, cols)\n",
    "\n",
    "                display_img = img[0].detach().cpu()\n",
    "                if display_img.shape[0] == 1:\n",
    "                    display_img = display_img.squeeze(0)\n",
    "                display_img = (display_img + 1) / 2\n",
    "                display_img = display_img.numpy()\n",
    "\n",
    "                axes[r, c].imshow(display_img, cmap=\"gray\")\n",
    "                if rows > 1 or cols > 1:\n",
    "                    axes[r, c].set_title(f\"Sample {idx+1}\")\n",
    "                axes[r, c].set_xticks([])\n",
    "                axes[r, c].set_yticks([])\n",
    "\n",
    "            # Add a global title\n",
    "            fig.suptitle(f\"Diffusion Process - Step {T-i}/{T}\", fontsize=16, y=0.98)\n",
    "\n",
    "            # Save the frame\n",
    "            frame_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.png\")\n",
    "            plt.savefig(frame_path, dpi=150, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count % 10 == 0:\n",
    "                print(f\"Saved {frame_count} frames\")\n",
    "\n",
    "        # Now perform denoising step for all images\n",
    "        for idx in range(len(grid_images)):\n",
    "            t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "            grid_images[idx] = sample_timestep(grid_images[idx], t)\n",
    "            grid_images[idx] = torch.clamp(grid_images[idx], -1.0, 1.0)\n",
    "\n",
    "    # Save final frame\n",
    "    if rows == 1 and cols == 1:\n",
    "        fig, axes = plt.subplots(figsize=(6, 7))\n",
    "        axes = np.array([[axes]])\n",
    "    else:\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3 + 1))\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, cols)\n",
    "        elif cols == 1:\n",
    "            axes = axes.reshape(rows, 1)\n",
    "\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "\n",
    "    for idx, img in enumerate(grid_images):\n",
    "        r, c = divmod(idx, cols)\n",
    "        display_img = img[0].detach().cpu()\n",
    "        if display_img.shape[0] == 1:\n",
    "            display_img = display_img.squeeze(0)\n",
    "        display_img = (display_img + 1) / 2\n",
    "        display_img = display_img.numpy()\n",
    "\n",
    "        axes[r, c].imshow(display_img, cmap=\"gray\")\n",
    "        if rows > 1 or cols > 1:\n",
    "            axes[r, c].set_title(f\"Sample {idx+1}\")\n",
    "        axes[r, c].set_xticks([])\n",
    "        axes[r, c].set_yticks([])\n",
    "\n",
    "    fig.suptitle(\"Final Result\", fontsize=16, y=0.98)\n",
    "\n",
    "    frame_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.png\")\n",
    "    plt.savefig(frame_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved {frame_count+1} frames to {output_dir}\")\n",
    "    print(\"You can combine these frames into a video using FFmpeg:\")\n",
    "    print(\n",
    "        f\"ffmpeg -framerate 30 -i {output_dir}/frame_%04d.png -c:v libx264 -pix_fmt yuv420p diffusion_video.mp4\"\n",
    "    )\n",
    "\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_diffusion_video(grid=(4, 4), show_every=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "    <video alt=\"test\" controls>\n",
    "        <source src=\"diffusion_video.mp4\" type=\"video/mp4\" width=500>\n",
    "    </video>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
